<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="openvtuber Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="openvtuber Blog Atom Feed"><title data-react-helmet="true">Facial Detection, Tracking, and Landmarking | openvtuber</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Facial Detection, Tracking, and Landmarking | openvtuber"><meta data-react-helmet="true" name="description" content="The ML module performs facial detection, tracking, and landmarking (in that order) on the image input it receives. One can find all the functions related to image and facial processing in the Inference class in ml.py. The core logic is within the infer_image function at the bottom. It takes an image and transforms it, producing a tuple containing measurements about the face."><meta data-react-helmet="true" property="og:description" content="The ML module performs facial detection, tracking, and landmarking (in that order) on the image input it receives. One can find all the functions related to image and facial processing in the Inference class in ml.py. The core logic is within the infer_image function at the bottom. It takes an image and transforms it, producing a tuple containing measurements about the face."><meta data-react-helmet="true" property="og:url" content="https://virtuber.github.io/docs/ml"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://virtuber.github.io/docs/ml"><link rel="stylesheet" href="/styles.21f0090d.css">
<link rel="preload" href="/styles.fd3ca948.js" as="script">
<link rel="preload" href="/runtime~main.cd1584fc.js" as="script">
<link rel="preload" href="/main.291a6b23.js" as="script">
<link rel="preload" href="/common.922b3625.js" as="script">
<link rel="preload" href="/10.f7084705.js" as="script">
<link rel="preload" href="/11.f56d88f7.js" as="script">
<link rel="preload" href="/935f2afb.7340e38d.js" as="script">
<link rel="preload" href="/17896441.7428816c.js" as="script">
<link rel="preload" href="/9c762327.18ada2cc.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">openvtuber</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/virtuber/openvtuber" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2N3Q"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_3NWk">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_3NWk">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">openvtuber</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs/">Docs</a></li><li class="menu__list-item"><a href="https://github.com/virtuber/openvtuber" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_vMrn"><div class="docSidebarContainer_3Ak5" role="complementary"><div class="sidebar_3gvy"><div class="menu menu--responsive thin-scrollbar menu_1yIk"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_1CUI" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Developers</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/">Getting Started</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/ml">Facial Detection, Tracking, and Landmarking</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docker">Running in Docker</a></li></ul></li></ul></div></div></div><main class="docMainContainer_2iGs"><div class="container padding-vert--lg docItemWrapper_1bxp"><div class="row"><div class="col docItemCol_U38p"><div class="docItemContainer_a7m4"><article><header><h1 class="docTitle_Oumm">Facial Detection, Tracking, and Landmarking</h1></header><div class="markdown"><p>The ML module performs facial detection, tracking, and landmarking (in that order) on the image input it receives. One can find all the functions related to image and facial processing in the Inference class in <code>ml.py</code>. The core logic is within the infer_image function at the bottom. It takes an image and transforms it, producing a tuple containing measurements about the face.  </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="inference"></a>Inference<a class="hash-link" href="#inference" title="Direct link to heading">#</a></h2><p>The inference process contains four steps, the first step is the initialisation process. The inference module relies heavily on dlib functions for much of its actual core actions, many of which need prior initialisation or preparation of some sort.  </p><p>The next part is where it actually takes in an image and begins the processing. It will look for a face in the image. If a face is not found, it will not go through with the remaining steps. Dlib functions performs the calculations for this part and this is probably the slowest part of the algorithm. Although there are other choices available online for this part, dlib is one of the most well known and most used.  </p><p>If it finds a face, it will take the cut out of the image that contains just the face and record its position. The algorithm does this so that it does not have to perform facial detection on every frame. This is because by recording past frames, we can actually use linear extrapolation to predict future face locations. Since facial detection is the slowest part in this entire algorithm, by being able to skip it occasionally, you can see noticeable performance boosts. In the current implementation, the algorithm skips every second frame and replaces it with this linear extrapolation approach.  </p><p>Once the algorithm detects a face, it crops and isolates the face so that it can then perform facial landmarking. This increases the accuracy and speed of the facial landmarking procedure tremendously. This process also takes advantage of an algorithm available through dlib that performs facial landmarking using 68 points.  </p><p>With the face landmarked, one can now use the points to calculate facial metrics.  </p><p>The entire process and algorithm is in the infer_image function at the bottom of <code>ml.py</code>.  </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="inference-output"></a>Inference Output<a class="hash-link" href="#inference-output" title="Direct link to heading">#</a></h2><p><strong>infer_image(image)</strong><br>
<strong>Input</strong>: 2D numpy array that represents an image<br>
<strong>Output</strong>: tuple of elements</p><ul><li>roll</li><li>pitch</li><li>yaw</li><li>ear_left</li><li>ear_right</li><li>mar</li><li>mdst</li><li>left_iris</li><li>right_iris</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="roll-pitch-yaw"></a>Roll, Pitch, Yaw<a class="hash-link" href="#roll-pitch-yaw" title="Direct link to heading">#</a></h3><p>These refer to the orientation of the head (in degrees). All three measurements start at a base line where 0 degrees is the subject staring straight into the camera.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="ear_left-ear_right"></a>ear_left, ear_right<a class="hash-link" href="#ear_left-ear_right" title="Direct link to heading">#</a></h3><p>Eye aspect ratio or ear is a measurement of how open an eye is. Values usually range from 0 to 0.4 where closed is 0 and very open is 0.4.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="mar"></a>mar<a class="hash-link" href="#mar" title="Direct link to heading">#</a></h3><p>Mouth aspect ratio or mar is a measurement of how open the mouth is. Values usually range from 0 to 1 (although it will sometimes cross 1) where closed is 0 and open is 1.  </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="mdst"></a>mdst<a class="hash-link" href="#mdst" title="Direct link to heading">#</a></h3><p>Mouth distance or mdst is a measure of the horizontal length of the mouth. (Think of it like a very wide smile versus a puckered kissing action). Values will usually range from 0.2 to 0.5 (although it may cross those boundaries on either side) where 0.2 is a very tight mouth and 0.5 is a very wide mouth.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="left_iris-right_iris"></a>left_iris, right_iris<a class="hash-link" href="#left_iris-right_iris" title="Direct link to heading">#</a></h3><p>These two iris measurements are not actually numbers but instead an array or collection of four other numbers.  </p><p>The first two of which are the raw x and y coordinates of the iris in relation to the rest of the image. These two numbers are probably not going to be very useful for most cases except for debugging. The coordinate (0, 0) is the upper left hand of the image.  </p><p>The next two of which are the left and up ratio values of the pupil. These represent how much to any direction the iris is pointing. The left ratio of the eye determines how much to the left the eye is pointing where 0 is very left and 1 is very right. The up ratio of the eye determines how upwards the eye is pointing where 0 is very down and 1 is very up.</p><p><strong>*Note</strong>: If it does not find a face, the function returns None instead.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/virtuber/openvtuber/edit/master/docs/docs/ml.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Getting Started</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/docker"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Running in Docker Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#inference" class="table-of-contents__link">Inference</a></li><li><a href="#inference-output" class="table-of-contents__link">Inference Output</a><ul><li><a href="#roll-pitch-yaw" class="table-of-contents__link">Roll, Pitch, Yaw</a></li><li><a href="#ear_left-ear_right" class="table-of-contents__link">ear_left, ear_right</a></li><li><a href="#mar" class="table-of-contents__link">mar</a></li><li><a href="#mdst" class="table-of-contents__link">mdst</a></li><li><a href="#left_iris-right_iris" class="table-of-contents__link">left_iris, right_iris</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/ml/">ML</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 virtuber</div></div></div></footer></div>
<script src="/styles.fd3ca948.js"></script>
<script src="/runtime~main.cd1584fc.js"></script>
<script src="/main.291a6b23.js"></script>
<script src="/common.922b3625.js"></script>
<script src="/10.f7084705.js"></script>
<script src="/11.f56d88f7.js"></script>
<script src="/935f2afb.7340e38d.js"></script>
<script src="/17896441.7428816c.js"></script>
<script src="/9c762327.18ada2cc.js"></script>
</body>
</html>